{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "AdaBoost on a uni-dimensional array\n",
    "\n",
    "Given the dataset below and the AdaBoost algorithm using the usual decision stumps as weak learners:\n",
    "\n",
    "1. Plot the dataset using `pyplot`.\n",
    "2. Draw the decision surface corresponding to the first weak learner.\n",
    "3. What are the values of $\\epsilon_1$ (training error of the first decision stump) and $\\alpha_1$ (the \"weight\" of the vode of the first decision stump)?\n",
    "4. What will be the updated weights of the training instances, after the first update?\n",
    "5. Draw the decision surface after adding the second weak learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "d = pd.DataFrame({\n",
    "    'X': [-1, -0.7, -0.4, -0.1, 0.2, 0.5, 0.8],\n",
    "    'Y': [1, 1, 1, -1, -1, -1, 1]\n",
    "})\n",
    "X, Y = d[['X']], d['Y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "AdaBoost on a two-dimensional array\n",
    "\n",
    "Given the dataset below and the AdaBoost algorithm using the usual decision stumps as weak learners:\n",
    "1. Plot the dataset using `pyplot`.\n",
    "2. Draw the decision surface corresponding to the first weak learner as chosen by `AdaBoostClassifier` with the default `base_estimator`.\n",
    "3. Show why AdaBoost chose that learner, by plotting the decision surface of all the candidates and their corresponding error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "d = pd.DataFrame({\n",
    "    'X1': [1, 2, 2.75, 3.25, 4, 5],\n",
    "    'X2': [1, 2, 1.25, 2.75, 2.25, 3.5],\n",
    "    'Y': [1, 1, -1, 1, -1, -1]\n",
    "})\n",
    "X, Y = d[['X1', 'X2']], d['Y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "AdaBoost vs ID3\n",
    "\n",
    "Given the dataset below:\n",
    "1. Plot the dataset using `pyplot`.\n",
    "2. Compare the training error of the AdaBoost algorithm (using the usual decision stumps as weak learners) and the ID3 algorithm.\n",
    "2. Compare the CVLOO error of the AdaBoost algorithm (using the usual decision stumps as weak learners) and the ID3 algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "x_red = norm.rvs(0, 1, 100, random_state=1)\n",
    "y_red = norm.rvs(0, 1, 100, random_state=2)\n",
    "x_green = norm.rvs(1, 1, 100, random_state=3)\n",
    "y_green = norm.rvs(1, 1, 100, random_state=4)\n",
    "d = pd.DataFrame({\n",
    "    'X1': np.concatenate([x_red,x_green]),\n",
    "    'X2': np.concatenate([y_red,y_green]),\n",
    "    'Y': [1]*100+[0]*100\n",
    "})\n",
    "X, Y = d[['X1', 'X2']], d['Y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "Finding the optimum number of weak learners\n",
    "\n",
    "For the dataset below:\n",
    "1. plot the points using `pyplot.scatter`;\n",
    "1. plot a line chart using `pyplot.plot` that shows the training error and the CVLOO error of AdaBoost using between 1 and 15 weak learners.\n",
    "1. What is the best number of weak learners in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "x_red = norm.rvs(0, 1, 100, random_state=1)\n",
    "y_red = norm.rvs(0, 1, 100, random_state=2)\n",
    "x_green = norm.rvs(1, 1, 100, random_state=3)\n",
    "y_green = norm.rvs(1, 1, 100, random_state=4)\n",
    "d = pd.DataFrame({\n",
    "    'X1': np.concatenate([x_red,x_green]),\n",
    "    'X2': np.concatenate([y_red,y_green]),\n",
    "    'Y': [1]*100+[0]*100\n",
    "})\n",
    "X, Y = d[['X1', 'X2']], d['Y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5\n",
    "Agglomerative clustering on a 2d dataset\n",
    "\n",
    "Considering the points (-4, -2), (-3, -2), (-2, -2), (-1, -2), (1, -1), (1, 1), (2, 3), (3, 2), (3, 4), (4, 3):\n",
    "1. create a scatter plot using `pyplot`;\n",
    "1. create the dendrogram using `AgglomerativeClustering` with single-linkage and then color the scatter plot using the best 4 clusters;\n",
    "1. create the dendrogram using `AgglomerativeClustering` with complete-linkage and then color the scatter plot using the best 4 clusters;\n",
    "1. what is the difference in behaviour between the two types of linkage? What shapes do they tend to give to the clusters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6\n",
    "\"Natural\" clusters\n",
    "\n",
    "Given the dataset {0, 4, 5, 20, 25, 39, 43, 44}:\n",
    "1. find the natural clusters using agglomerative clusters with single-linkage and plot clusters using a scatter plot;\n",
    "1. find the natural clusters using agglomerative clusters with average-linkage and plot clusters using a scatter plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7\n",
    "\n",
    "For the following two datasets `d1` and `d2`:\n",
    "1. plot the points using `pyplot` and highlight (by using different colours for the points) the 2 clusters found by agglomerative clustering using single linkage;\n",
    "1. plot the points using `pyplot` and highlight (by using different colours for the points) the 2 clusters found by agglomerative clustering using average linkage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets as ds\n",
    "\n",
    "np.random.seed(0)\n",
    "X1, _ = ds.make_circles(n_samples=1500, factor=.5,  noise=.05)\n",
    "X2, _ = ds.make_blobs(n_samples=1500,\n",
    "                      cluster_std=[1.0, 2.5, 0.5],\n",
    "                      random_state=170)\n",
    "\n",
    "d1 = pd.DataFrame(X1, columns=['X1', 'X2'])\n",
    "d2 = pd.DataFrame(X2, columns=['X1', 'X2'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
